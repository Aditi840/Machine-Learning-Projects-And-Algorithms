# -*- coding: utf-8 -*-
"""KMeans(Hands On)US.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pez016drl26XurUb-jDqZNVKFkEax2L-
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# %matplotlib inline
sns.set_context('notebook')
plt.style.use('fivethirtyeight')
from warnings import filterwarnings
filterwarnings('ignore')

df = pd.read_csv('/content/faithful.csv')
df
del df['Unnamed: 0']
df.describe()
df.isnull().sum()
df.dtypes
df = df.drop_duplicates()

plt.figure(figsize=(8,8))
plt.scatter(df.iloc[:,0], df.iloc[:,1])
plt.xlabel('Eruption time in mins')
plt.ylabel('Waiting time to next eruption')
plt.title('Visualisation of raw data');

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_std = scaler.fit_transform(df)

from sklearn.cluster import KMeans
km = KMeans(n_clusters=2, random_state=42) ## applying k = 2
km.fit(X_std) # fit the data - identify pattern, does everything

centroids = km.cluster_centers_ # final centroid points
print(centroids)
print(km.labels_) # measures how tight my groups are.
print(km.inertia_)

fig, ax = plt.subplots(figsize=(6,6))
plt.scatter(X_std[km.labels_ == 0,0], X_std[km.labels_ == 0,1],
            c = 'green', label = 'cluster 1')
plt.scatter(X_std[km.labels_ == 1,0], X_std[km.labels_ == 1,1],
            c = 'blue', label = 'cluster 2')
plt.scatter(centroids[:,0], centroids[:,1], marker = '*', s=300,
            c = 'red', label = 'centroid')

plt.legend()
plt.xlim([-2,2])
plt.ylim([-2,2])
plt.xlabel('Eruption time in mins')
plt.ylabel('Waiting time for next eruption')
plt.title('Visualisation of clustered data', fontweight = 'bold')

km = KMeans(n_clusters=4) # k = 4
km.fit(X_std)
centroids = km.cluster_centers_
#centroids
print(km.inertia_) # measures how tight my clusters are -> lower the better (lower => the clusters are more tight)

# Plot the clustered data
fig, ax = plt.subplots(figsize=(6, 6))
plt.scatter(X_std[km.labels_ == 0, 0], X_std[km.labels_ == 0, 1],
            c='green', label='cluster 1')
plt.scatter(X_std[km.labels_ == 1, 0], X_std[km.labels_ == 1, 1],
            c='blue', label='cluster 2')
plt.scatter(X_std[km.labels_ == 2, 0], X_std[km.labels_ == 2, 1],
            c='orange', label='cluster 3')
plt.scatter(X_std[km.labels_ == 3, 0], X_std[km.labels_ == 3, 1],
            c='yellow', label='cluster 4')
plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,
            c='red', label='centroid')
plt.legend()
plt.xlim([-3, 2])
plt.ylim([-3, 2])
plt.xlabel('Eruption time in mins')
plt.ylabel('Waiting time to next eruption')
plt.title('Visualization of clustered data', fontweight='bold')
ax.set_aspect('equal');

"""Impact of Initialisation and Randomness"""

import numpy as np
n_iter = 9
fig, ax = plt.subplots(3, 3, figsize=(16, 16))
ax = np.ravel(ax)
centers = []
for i in range(n_iter):
    seed = np.random.randint(0, 100000, size=1)[0]
    print(seed)
    km = KMeans(n_clusters=3,max_iter=3,init = 'random',n_init=1,random_state=np.random.RandomState(seed))
    km.fit(X_std)
    centroids = km.cluster_centers_
    centers.append(centroids)
    ax[i].scatter(X_std[km.labels_ == 0, 0], X_std[km.labels_ == 0, 1],
                  c='green', label='cluster 1')
    ax[i].scatter(X_std[km.labels_ == 1, 0], X_std[km.labels_ == 1, 1],
                  c='blue', label='cluster 2')
    ax[i].scatter(X_std[km.labels_ == 2, 0], X_std[km.labels_ == 2, 1],
                  c='yellow', label='cluster 3')
    ax[i].scatter(centroids[:, 0], centroids[:, 1],
                  c='r', marker='*', s=300, label='centroid')
    ax[i].set_xlim([-2, 2])
    ax[i].set_ylim([-2, 2])
    ax[i].legend(loc='lower right')
    ax[i].set_title(f'{km.inertia_:.4f}')
    ax[i].set_aspect('equal')
plt.tight_layout();

"""Elbow plot for finding best value of k"""

# Elbow plot
# We make a plot between k value and inertia
inertias = [] # storer inertia for all values of k
list_k = list(range(1,10))

for k in list_k:
  km = KMeans(n_clusters= k)
  km.fit(X_std)
  inertias.append(km.inertia_)

#Plot see against k
plt.figure(figsize = (6,6))
plt.plot(list_k, inertias, '-o')
plt.xlabel(r'Number of clusters *k*')
plt.ylabel('Inertia');

"""Drawbacks of KMeans

Does not let the data point far off from each other to be in the same cluster
"""

import numpy as np
X = np.tile(np.linspace(1,5,20), 2)
y = np.repeat(np.array([2,4]), 20)
df = np.c_[X, y]

plt.scatter(X,y)

km = KMeans(n_clusters=2)
km.fit(df)
labels = km.predict(df)
centroids = km.cluster_centers_

fig,ax = plt.subplots(figsize= (6,6))
plt.scatter(X, y, c = labels)
plt.xlim([0, 6])
plt.ylim([0, 6])
plt.text(centroids[0][0], centroids[0][1], 'A', color='red')
plt.text(centroids[1][0], centroids[1][1], 'B', color='red')
ax.set_aspect('equal')

"""Drawback of KMeans clustering algorithm is that it can only find spherical patterns in the data"""

from sklearn.datasets import make_circles, make_moons

X1 = make_circles(factor = 0.5, noise = 0.05, n_samples = 1500)

X2 = make_moons(n_samples = 1500, noise = 0.05)

plt.scatter(X1[0][:, 0], X1[0][:, 1])

fig, ax = plt.subplots(1,2)
for i,X  in enumerate([X1]):
  fig.set_size_inches(18,7)
  km = KMeans(n_clusters = 2)
  km.fit(X[0])
  labels = km.predict(X[0])
  centroids = km.cluster_centers_

  ax[i].scatter(X[0][:, 0], X[0][:, 1], c = labels)
  ax[i].scatter(centroids[0,0], centroids[0,1], marker = '*', s=400, c = 'r')
  ax[i].scatter(centroids[1,0], centroids[1,1], marker = '+', s=300, c='green')
plt.suptitle('Simulated data', y = 1.05, fontsize = 22, fontweight = 'semibold')
plt.tight_layout()