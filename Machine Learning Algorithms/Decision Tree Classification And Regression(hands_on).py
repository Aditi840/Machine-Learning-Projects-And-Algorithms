# -*- coding: utf-8 -*-
"""Decision Tree classification and Regression(Hands On).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19kUVN4h-BKJFr9MufqF8hSjNcUI-J236
"""

import pandas as pd
wbc = pd.read_csv('/content/wbc.csv')
wbc.head()

wbc.describe()

wbc.isnull().sum()

wbc.shape

del wbc['Unnamed: 32']

del wbc['id']

wbc.isnull().sum()

wbc.dtypes

wbc = wbc.drop_duplicates()

wbc

X = wbc.loc[:, wbc.columns[1:]]
y = wbc['diagnosis']
y = y.map({'M':1, 'B':0})

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

"""Decision Tree Fitting"""

from sklearn.tree import DecisionTreeClassifier

#Intialize a DecisionTreeClassifier 'dt' with a maximum depth
dt = DecisionTreeClassifier()

#Fit
dt.fit(X_train, y_train) # it will ask all possible questions, compute the information gain and choose the best split

#Predict test set labels
y_pred = dt.predict(X_test)
y_pred

"""Evaluating a Decision Tree"""

from sklearn.metrics import accuracy_score, roc_auc_score, f1_score

from sklearn.metrics import roc_curve, roc_auc_score

#we compute the eval metric on test/validation set only primarily

# Predict test set labels
y_pred = dt.predict(X_test)

# Compute test set accuracy
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print("Test set accuracy: {:.2f}".format(acc))
print("Test set f1-score: {:.2f}".format(f1))

aucc = roc_auc_score(y_test, y_pred)
print("Test set auc: {:.2f}".format(aucc))



auc = roc_auc_score(y_train, dt.predict(X_train))
print("Train set AUC : ", auc)

from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

# Assuming 'dt' is your decision tree classifier and 'X_test' and 'y_test' are your test data
fpr, tpr, thresholds = roc_curve(y_test, dt.predict_proba(X_test)[:, 1])

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""

> Reducing Overfitting

"""

from sklearn.model_selection import cross_val_score
import numpy as np

for depth in [1,2,3,4,5,6,7,8,9,10,20]:
  dt = DecisionTreeClassifier(max_depth=depth) # will tell the DT to not grow past the given threhsold
  #Fit dt to the training dataset
  dt.fit(X_train, y_train) # the model is trained
  trainAccuracy = accuracy_score(y_train, dt.predict(X_train))
  dt = DecisionTreeClassifier(max_depth=depth) # a fresh model which is not trained yet
  valAccuracy = cross_val_score(dt, X, y, cv=10) # syntax : cross_val_Score(freshModel,fts, target, cv= 10/5)
  print("Depth : ",depth, " Training Accuracy : ", trainAccuracy, " Cross val score : ", np.mean(valAccuracy))

"""Visualising the decision boundary"""

!pip install mlxtend==0.22.0

from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt

def plot_labeled_decision_regions(X,y, models):
    '''Function producing a scatter plot of the instances contained
    in the 2D dataset (X,y) along with the decision
    regions of two trained classification models contained in the
    list 'models'.

    Parameters
    ----------
    X: pandas DataFrame corresponding to two numerical features
    y: pandas Series corresponding the class labels
    models: list containing two trained classifiers

    '''
    if len(models) != 2:
        raise Exception('''Models should be a list containing only two trained classifiers.''')
    if not isinstance(X, pd.DataFrame):
        raise Exception('''X has to be a pandas DataFrame with two numerical features.''')
    if not isinstance(y, pd.Series):
        raise Exception('''y has to be a pandas Series corresponding to the labels.''')
    fig, ax = plt.subplots(1, 2, figsize=(10.0, 5), sharey=True)
    for i, model in enumerate(models):
        plot_decision_regions(X.values, y.values, model, legend= 2, ax = ax[i])
        ax[i].set_title(model.__class__.__name__)
        ax[i].set_xlabel(X.columns[0])
        if i == 0:
            ax[i].set_ylabel(X.columns[1])
            ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())
            ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())
    plt.tight_layout()

X_train

from sklearn.linear_model import LogisticRegression

dt = DecisionTreeClassifier(max_depth=4)
X_train = X_train.loc[:, ['radius_mean', 'texture_mean']]
X_test = X_test.loc[:,['radius_mean', 'texture_mean']]
dt.fit(X_train, y_train)

#Instantiate logreg
logreg = LogisticRegression(random_state=1)
logreg.fit(X_train, y_train)

clfs = [logreg, dt]

#Review the decision regions of the two classifier
plot_decision_regions(X_train.values, y_train.values, dt)

X_train.columns

"""Feature Importances"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from matplotlib import pyplot
dt = DecisionTreeClassifier(max_depth=5)
#Fit dt to the training set
dt.fit(X_train, y_train)
importance = dt.feature_importances_
#pyplot.bar([x for x in range(len(importance))], importance)
list(zip(importance, X_test.columns)) # it calculates the feature importances based on IG

list(zip([1,2,3],[4,5,6]))

X.columns

"""Visualising the trees"""

from sklearn import tree
tree.plot_tree(dt)

import matplotlib.pyplot as plt
fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (4,4), dpi = 300)
tree.plot_tree(dt,
               feature_names = X_test.columns,
               filled = True);
fig.savefig('imagename.png')

dt.predict_proba(X_test)

"""Decision Trees Regressor"""

import pandas as pd
from sklearn.model_selection import train_test_split

mpg = pd.read_csv('/content/auto-mpg.csv')
mpg.head()
del mpg['car name']
mpg.dtypes
mpg.horsepower.unique()
mpg = mpg.loc[mpg.horsepower != '?', :]
mpg.horsepower.unique()
mpg.horsepower = mpg.horsepower.astype('int')
mpg.isna().sum()
mpg = mpg.drop_duplicates()
mpg.describe()

X = mpg.drop('mpg', axis='columns')
y = mpg['mpg']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

X_train

"""**Decision Tree Regression**"""

from sklearn.tree import DecisionTreeRegressor

#Instantiate dt
dt = DecisionTreeRegressor()

#Fit dt to the training set
dt.fit(X_train, y_train)
dt.predict(X_test)

"""Evaluation Metrics"""

from sklearn.metrics import r2_score

#compute y_pred
y_pred = dt.predict(X_test)

#compute MSE dt
rsquared = r2_score(y_test, y_pred)
#compute rmse_dt

#print rmse_dt
print("Test set R2 of dt: {:.2f}".format(rsquared))

"""Linear Regression Vs Decision Tree Regression"""

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)

#compute mse_lr
rsquared = r2_score(y_test, y_pred_lr)
#compute rmse_lr

#print rmse_lr
print("Linear Regression test set R2: {:.2f}".format(rsquared))

#print rmse_dt
print("Linear Regression Score : ", r2_score(y_test, y_pred_lr))
print("Regression Tree's Score : ", r2_score(y_test, y_pred))

"""Reducing Overfitting"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer
import numpy as np

for depth in [1,2,3,4,5,6,7,8,9,10,20,40,60]:
  dt = DecisionTreeRegressor(max_depth=depth) #will tell the DT to not grow past the given threhsold
  #Fit dt to the training set
  dt.fit(X_train, y_train)
  trainAccuracy = r2_score(y_train, dt.predict(X_train))
  dt = DecisionTreeRegressor(max_depth=depth) # a fresh model which is not trained yet
  valAccuracy = cross_val_score(dt, X_train, y_train, cv=10, scoring = make_scorer(r2_score)) # syntax : cross_val_Score(freshModel,fts, target, cv= 10/5)
  print("Depth : ", depth," Train R2 : ",trainAccuracy, " Val Score : ", np.mean(valAccuracy))

"""Feature Importances"""

from matplotlib import pyplot
dt = DecisionTreeRegressor(max_depth = 5)
# Fit dt to the training set
dt.fit(X_train, y_train)
importance = dt.feature_importances_
#pyplot.bar([x for x in range(len(importance))], importance)
list(zip(importance,X_test.columns))

X_train.columns

from matplotlib import pyplot
dt = DecisionTreeRegressor(max_depth=3)
X_train = X_train.loc[:, ['horsepower', 'displacement', 'model year']]
X_test = X_test.loc[:, ['horsepower', 'displacement', 'model year']]
# Fit dt to the training set
dt.fit(X_train, y_train)
r2_score(dt.predict(X_test),y_test)

"""Visualising the tree"""

import matplotlib.pyplot as plt
from sklearn import tree

fig, axes = plt.subplots(nrows=1, ncols=1, figsize = (4,4), dpi = 300)
tree.plot_tree(dt,
               feature_names = X_test.columns,
               filled = True),
fig.savefig('imagename.png')