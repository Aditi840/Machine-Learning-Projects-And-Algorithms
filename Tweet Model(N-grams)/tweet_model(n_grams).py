# -*- coding: utf-8 -*-
"""Tweet Model(N-Grams).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FQT-iZMrRKNu_wbqLnGn6mIzT_cZTWFC

# Importing Libraries
"""

pip install markovify

import numpy as np
import pandas as pd
import re
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('punkt')
import math
from matplotlib.pyplot import plot
import random
import markovify

"""# Scrape data from twitter using snscrape"""



"""# Importing dataset"""

df = pd.read_csv('/content/TweetsElonMusk.csv')

df['tweet'].head()

tweets = np.array(df['tweet'])

tweets[1]

1

"""# Pre-Processing"""

tweets[5]

stop = stopwords.words('english')

def preprocessing(tweets):

  _tweets = []
  for tweet in tweets:

    #replace @mentions should be replaced with user
    tweet = re.sub(r'@\w+','_user',tweet)

    #replacing all the "links" with links
    tweet = re.sub(r'http\S','links',tweet)

    #remove all the punctuation
    tweet = re.sub(r'[^\w\s]','',tweet)

    #remove numbers
    tweet = ''.join([i for i in tweet if not i.isdigit()])

    #make all the words in tweets in lowercase letter
    tweet = tweet.lower()

    #tokenize and removing stopwords
    tweet = word_tokenize(tweet)
    tweet = [word for word in tweet if word not in stop]

    _tweets.append(tweet)

  return _tweets

_tweets = preprocessing(tweets)

_tweets[0],tweets[0]

X = int(len(_tweets)/10 * 9)

#splitting out training and test data from _tweets
trainData = _tweets[0:X]
testData = _tweets[X:len(_tweets)]

len(trainData),len(testData)

"""# N-grams"""

#hyper-parameters
N = 3
k = 0.01

SOS = "<s>"
EOS = "</s>"

def preprocess(trainData,N):
  """Each sentence is added with <s> and </s>.
  for N>=2 number of SOS would be N-1
  but suppose N=1 SOS would be
  and EOS would be one in every case"""
  trainDatatemp = []
  for sen in trainData:
    temp_sen = []
    if N==1:
      temp_sen.append(SOS)
    else:
      for i in range(N-1):
        temp_sen.append(SOS)

    temp_sen.extend(sen)
    temp_sen.append(EOS)
    trainDatatemp.append(temp_sen)

  return trainDatatemp

_trainData = preprocess(trainData,N)

_trainData[0],trainData[0]

all_tokens = []
for tweet in _trainData:
  all_tokens.extend(tweet)

len(all_tokens)

vocab = nltk.FreqDist(all_tokens)
vocab

len(trainData)

vocab[EOS]

def NGramModel(N,all_tokens,vocab):

  if N==1:
    #we need our unigram model
    totalTokens = len(all_tokens)
    tempDict = {}
    for uniG,total in vocab.items():
      tempDict[uniG] = total/totalTokens

  else:

    totalNGrams = nltk.ngrams(all_tokens,N)
    totalNDict = nltk.FreqDist(totalNGrams)

    totalMGrams = nltk.ngrams(all_tokens,N-1)
    totalMDict = nltk.FreqDist(totalMGrams)

    vocabsize = len(vocab)

    tempDict = {}
    for nGram,count in totalNDict.items():
      tempDict[nGram] = Laplace(nGram,count,vocab,totalMGrams,totalMDict,vocabsize)

  return tempDict

def Laplace(nGram,nCount,vocab,totalMGrams,totalMDict,V):
  mGram = nGram[:-1] # SOS SOS MY -- Ngram, # SOS SOS Mgram
  mCount = totalMDict[mGram]
  return (nCount + k) / (mCount + k * V)

model = NGramModel(N,all_tokens,vocab)

model

"""# Evaluation Model"""

#defining perplexity
def perplexity(testData, N):

  _testData = preprocess(testData,N)
  test_all_tokens = []
  for tweet in _testData:
    test_all_tokens.extend(tweet)

  totalTestNGrams = nltk.ngrams(test_all_tokens,N)
  total = len(test_all_tokens)

  probablities = []
  for ngram in totalTestNGrams:
    if ngram in model:
      probablities.append(model[ngram])

  ans = math.exp((-1/total) * sum(map(math.log,probablities))) #formula for perplexity
  return ans

#perPlex = {}

perPlex[N] = perplexity(testData,N)

perPlex

myKeys = list(perPlex.keys())
myKeys.sort()
sorted_dict = {i:perPlex[i] for i in myKeys}

X = list(sorted_dict.keys())
y = list(sorted_dict.values())

plot(X,y)

"""# Generate Tweets"""

# ... (your previous code)

# Print some N-grams and their associated probabilities for debugging
for ngram, probabilities in model.items():
    print(f"N-gram: {ngram}, Probabilities: {probabilities}")

# ... (the rest of your code)

import random

# Concatenate your pre-processed training data into a single text
training_text = " ".join(" ".join(tweet) for tweet in trainData)

# Generate tweets
generated_tweets = []

for _ in range(10):  # Generate 10 tweets
    tweet = random.sample(training_text.split(), random.randint(5, 20))
    generated_tweet = " ".join(tweet)
    generated_tweets.append(generated_tweet)

# Print the generated tweets
for i, tweet in enumerate(generated_tweets, start=1):
    print(f"Generated Tweet {i}: {tweet}")

# Define a function to generate tweets based on the N-gram model
def generate_tweets(model, N, num_tweets, max_length=30):
    generated_tweets = []

    for _ in range(num_tweets):
        tweet = [SOS] * (N - 1)  # Initialize with N-1 SOS tokens

        while len(tweet) < max_length:
            prev_words = tuple(tweet[-(N - 1):])

            if prev_words in model:
                next_word = random.choice(list(model[prev_words].keys()))
            else:
                # Handle the case where prev_words are not in the model
                # You can choose a random word from the vocabulary or use a fallback strategy
                next_word = random.choice(list(vocab.keys()))  # Choose a random word from the vocabulary

            if next_word == EOS:
                break
            tweet.append(next_word)

        generated_tweet = " ".join(tweet[(N - 1):])  # Remove N-1 initial SOS tokens
        generated_tweets.append(generated_tweet)

    return generated_tweets

# Generate tweets using the N-gram model
num_tweets_to_generate = 10  # Change the number as needed
generated_tweets = generate_tweets(model, N, num_tweets_to_generate)

# Print the generated tweets
for i, tweet in enumerate(generated_tweets, start=1):
    print(f"Generated Tweet {i}: {tweet}")